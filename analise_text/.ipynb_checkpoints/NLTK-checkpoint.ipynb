{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. Smith!', 'Glad to see you.']\n",
      "['Mr.', 'Smith', '!', 'Glad', 'to', 'see', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Mr. Smith! Glad to see you.\"\n",
    "print (sent_tokenize(text))\n",
    "print (word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk.tokenize.casual'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-42c8f45247a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasual\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"hello world :) !!!!! http://alink.com/page#part\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'nltk.tokenize.casual'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "text = \"hello world :) !!!!! http://alink.com/page#part\"\n",
    "print (word_tokenize(text))\n",
    "print (tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и\n",
      "в\n",
      "во\n",
      "не\n",
      "что\n",
      "он\n",
      "на\n",
      "я\n",
      "с\n",
      "со\n",
      "как\n",
      "а\n",
      "то\n",
      "все\n",
      "она\n",
      "так\n",
      "его\n",
      "но\n",
      "да\n",
      "ты\n",
      "к\n",
      "у\n",
      "же\n",
      "вы\n",
      "за\n",
      "бы\n",
      "по\n",
      "только\n",
      "ее\n",
      "мне\n",
      "было\n",
      "вот\n",
      "от\n",
      "меня\n",
      "еще\n",
      "нет\n",
      "о\n",
      "из\n",
      "ему\n",
      "теперь\n",
      "когда\n",
      "даже\n",
      "ну\n",
      "вдруг\n",
      "ли\n",
      "если\n",
      "уже\n",
      "или\n",
      "ни\n",
      "быть\n",
      "был\n",
      "него\n",
      "до\n",
      "вас\n",
      "нибудь\n",
      "опять\n",
      "уж\n",
      "вам\n",
      "ведь\n",
      "там\n",
      "потом\n",
      "себя\n",
      "ничего\n",
      "ей\n",
      "может\n",
      "они\n",
      "тут\n",
      "где\n",
      "есть\n",
      "надо\n",
      "ней\n",
      "для\n",
      "мы\n",
      "тебя\n",
      "их\n",
      "чем\n",
      "была\n",
      "сам\n",
      "чтоб\n",
      "без\n",
      "будто\n",
      "чего\n",
      "раз\n",
      "тоже\n",
      "себе\n",
      "под\n",
      "будет\n",
      "ж\n",
      "тогда\n",
      "кто\n",
      "этот\n",
      "того\n",
      "потому\n",
      "этого\n",
      "какой\n",
      "совсем\n",
      "ним\n",
      "здесь\n",
      "этом\n",
      "один\n",
      "почти\n",
      "мой\n",
      "тем\n",
      "чтобы\n",
      "нее\n",
      "сейчас\n",
      "были\n",
      "куда\n",
      "зачем\n",
      "всех\n",
      "никогда\n",
      "можно\n",
      "при\n",
      "наконец\n",
      "два\n",
      "об\n",
      "другой\n",
      "хоть\n",
      "после\n",
      "над\n",
      "больше\n",
      "тот\n",
      "через\n",
      "эти\n",
      "нас\n",
      "про\n",
      "всего\n",
      "них\n",
      "какая\n",
      "много\n",
      "разве\n",
      "три\n",
      "эту\n",
      "моя\n",
      "впрочем\n",
      "хорошо\n",
      "свою\n",
      "этой\n",
      "перед\n",
      "иногда\n",
      "лучше\n",
      "чуть\n",
      "том\n",
      "нельзя\n",
      "такой\n",
      "им\n",
      "более\n",
      "всегда\n",
      "конечно\n",
      "всю\n",
      "между\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "for w in stopwords.words(\"russian\"):\n",
    "    print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'run'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")\n",
    "#lemmatizer.lemmatize(\"better\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Russia', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('country', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Части речи\n",
    "words = nltk.word_tokenize(\"Russia is a country\")\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
